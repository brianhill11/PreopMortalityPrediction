{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "print(os.getenv(\"PYTHONPATH\"))\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.preprocessing import StandardScaler, Imputer, LabelEncoder\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, brier_score_loss, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, ElasticNetCV, ARDRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from fancyimpute import SoftImpute\n",
    "from ehr_utils import *\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False to hide IDs    \n",
    "verbose = False\n",
    "\n",
    "# number of cross validation folds (we should have this many models)\n",
    "num_cross_val_folds = 5\n",
    "\n",
    "# version of scikit-learn that was used to create model (should be in pickled model filename)\n",
    "sk_version = \"0.19.1\"\n",
    "\n",
    "## this variable is the column that we will use as the target variable for the model\n",
    "TARGET_VARIABLE = 'INPT_DEATH_YN'\n",
    "\n",
    "MIN_ASA_STATUS=1\n",
    "MAX_ASA_STATUS=5\n",
    "MIN_AGE=18\n",
    "MAX_AGE=89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filtered_f = \"main_Nov21_2017_Feb_13_2018.filtered.main.txt\"\n",
    "\n",
    "# experiment prefix sets the set of features to use in the model\n",
    "exp_prefix = \"preop_asa\"\n",
    "dir_to_save_files = os.path.join(\"/datavol/mortality_paper/\", exp_prefix)\n",
    "\n",
    "## set path to directory containing pickled models\n",
    "model_dir = dir_to_save_files\n",
    "\n",
    "if not os.path.exists(dir_to_save_files):\n",
    "    os.makedirs(dir_to_save_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(home_dir, main_filtered_f),sep=\"|\")\n",
    "print df.shape\n",
    "print len(df.columns)\n",
    "if verbose:\n",
    "    display(df.iloc[0:20, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop any rows that are exact copies of another row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df.shape\n",
    "df.drop_duplicates(inplace=True)\n",
    "print df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = os.path.join(dir_to_save_files, 'EHR_MAIN_FEATURES.csv')\n",
    "features_df = pd.read_csv(FEATURES_PATH)\n",
    "\n",
    "features_dict = {name:list(col.dropna()) for name,col in features_df.items()}\n",
    "print(features_dict.keys())\n",
    "\n",
    "final_features = features_dict['final_features']\n",
    "cat_to_drop = features_dict['cat_to_drop']\n",
    "outcome_vars = features_dict['outcome_vars']\n",
    "feat_to_drop = features_dict['feat_to_drop']\n",
    "cat_vars = features_dict['cat_vars']\n",
    "contin_vars = features_dict['contin_vars']\n",
    "bool_outcome_vars = features_dict['bool_outcome_vars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out surgeries that don't occur in RR or SM operating rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.LOCATION_GROUP.unique())\n",
    "#df = df[df['LOCATION_GROUP'].isin(['RR OR', 'SM OR','SM SC','SM OB OR','RR OB OR'])]\n",
    "print(df.shape)\n",
    "df = df[df['LOCATION_GROUP'].isin(['RR OR', 'SM OR', 'SM SC'])]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out surgeries that were not INPATIENT, SAME DAY ADMIT, EMERGENCY, or OVERNIGHT RECOVERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Shape before filtering out outpatient surgeries:\", df.shape\n",
    "#df = df[df['PATIENT_CLASS'].isin(['INPATIENT', 'SAME DAY ADMIT', 'EMERGENCY', 'OVERNIGHT RECOVERY'])]\n",
    "df = df[df['PAT_CLASS'].isin(['INPATIENT', 'SAME DAY ADMIT', 'EMERGENCY', 'OVERNIGHT RECOVERY'])]\n",
    "print \"Shape after filtering out outpatient surgeries:\", df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter based on ASA status, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print \"Shape before filtering out based on ASA_STATUS:\", df.shape\n",
    "    print \"ASA_STATUS mean:\", df.ASA_STATUS.mean()\n",
    "    df = df[(df[\"ASA_STATUS\"] <= MAX_ASA_STATUS) & (df[\"ASA_STATUS\"] >= MIN_ASA_STATUS)]\n",
    "    print \"Shape after filtering out based on ASA_STATUS:\", df.shape\n",
    "    print \"ASA_STATUS mean:\", df.ASA_STATUS.mean()\n",
    "except AttributeError:\n",
    "    pass\n",
    "print \"===================================\"\n",
    "print \"Mean age:\", df.AGE_LT_90.mean()\n",
    "print \"STD age:\", df.AGE_LT_90.std()\n",
    "df = df[(df[\"AGE_LT_90\"] <= MAX_AGE) & (df[\"AGE_LT_90\"] >= MIN_AGE)]\n",
    "print \"Mean age:\", df.AGE_LT_90.mean()\n",
    "print \"STD age:\", df.AGE_LT_90.std()\n",
    "print \"Shape after filtering out based on AGE_LT_90:\", df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check demographic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Number of Patients:\", df.shape[0]\n",
    "print \"Patients with in-hospital mortality: {} ({}%)\".format(df.INPT_DEATH_YN.value_counts()[1], df.INPT_DEATH_YN.value_counts(normalize=\"True\")[1]*100)\n",
    "print \"Mean age:\", df.AGE_LT_90.mean(), \" std:\", df.AGE_LT_90.std()\n",
    "print \"Number of female patients: {} ({}%)\".format(df[df[\"GENDER\"] == \"F\"].shape[0], df[df[\"GENDER\"] == \"F\"].shape[0]/float(df.shape[0])*100)\n",
    "\n",
    "try:\n",
    "    print \"Number of patients in RR OR: {} ({}%)\".format(df.LOCATION_GROUP.value_counts()[\"RR OR\"], df.LOCATION_GROUP.value_counts(normalize=\"True\")[\"RR OR\"]*100)\n",
    "    print \"Number of patients in SM OR: {} ({}%)\".format(df.LOCATION_GROUP.value_counts()[\"SM OR\"], df.LOCATION_GROUP.value_counts(normalize=\"True\")[\"SM OR\"]*100)\n",
    "    print \"Number of patients in SM SC: {} ({}%)\".format(df.LOCATION_GROUP.value_counts()[\"SM SC\"], df.LOCATION_GROUP.value_counts(normalize=\"True\")[\"SM SC\"]*100)\n",
    "except AttributeError:\n",
    "    pass\n",
    "print(\"=\"*40)\n",
    "try:\n",
    "    print \"ASA Status:\", df.ASA_STATUS.value_counts()\n",
    "    print \"ASA Status (%):\", (df.ASA_STATUS.value_counts()/df.shape[0])*100\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "print(\"=\"*40)\n",
    "print \"Mean age of mortalities:\", df[df[\"INPT_DEATH_YN\"] == 1].AGE_LT_90.mean(), \" std:\", df[df[\"INPT_DEATH_YN\"] == 1].AGE_LT_90.std()\n",
    "print(\"Number of female mortalities: {} ({}%)\".format(df[df[\"GENDER\"] == \"F\"][\"INPT_DEATH_YN\"].sum(), \n",
    "                                                     df[df[\"GENDER\"] == \"F\"][\"INPT_DEATH_YN\"].sum()/float(df[\"INPT_DEATH_YN\"].sum())*100))\n",
    "print(\"Number of male mortalities: {} ({}%)\".format(df[df[\"GENDER\"] == \"M\"][\"INPT_DEATH_YN\"].sum(), \n",
    "                                                     df[df[\"GENDER\"] == \"M\"][\"INPT_DEATH_YN\"].sum()/float(df[\"INPT_DEATH_YN\"].sum())*100))\n",
    "try:\n",
    "    print(\"=\"*40)\n",
    "    print(\"Number of mortalities stratified by location\")\n",
    "    print(df.groupby(\"LOCATION_GROUP\")[\"INPT_DEATH_YN\"].sum())                                \n",
    "    print(df.groupby(\"LOCATION_GROUP\")[\"INPT_DEATH_YN\"].sum()/float(df[\"INPT_DEATH_YN\"].sum())*100)\n",
    "except AttributeError:\n",
    "    pass\n",
    "print(\"=\"*40)\n",
    "print(\"Number of mortalities stratified by ASA status\")\n",
    "print(df.groupby(\"ASA_STATUS\")[\"INPT_DEATH_YN\"].sum())\n",
    "print(df.groupby(\"ASA_STATUS\")[\"INPT_DEATH_YN\"].sum()/float(df[\"INPT_DEATH_YN\"].sum())*100)\n",
    "\n",
    "if verbose:\n",
    "    print(df[\"CASE_SRV_NAME\"].value_counts())\n",
    "    print(df[\"CASE_SRV_NAME\"].value_counts()/df.shape[0])*100\n",
    "    print(\"=\"*40)\n",
    "    print(df[df[\"INPT_DEATH_YN\"] == 1][\"CASE_SRV_NAME\"].value_counts())\n",
    "    print(df[df[\"INPT_DEATH_YN\"] == 1][\"CASE_SRV_NAME\"].value_counts()/df[\"INPT_DEATH_YN\"].sum())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = ['PRE_SURG_LOCATION', 'CASE_SRV_NAME_GROUP', 'CASE_SRV_NAME', 'PRIMARY_CPT',\n",
    "                                  'GENDER', 'HCUP_DESC', 'CPT_DESC', 'PAT_CLASS', 'OR_CASE_ID', 'ADMSN_ID']\n",
    "dff = df.drop(['PRE_SURG_LOCATION', 'CASE_SRV_NAME_GROUP', 'CASE_SRV_NAME', 'PRIMARY_CPT',\n",
    "                                  'GENDER', 'HCUP_DESC', 'CPT_DESC', 'PAT_CLASS', 'OR_CASE_ID', 'ADMSN_ID'], axis=1)\n",
    "dff = df.select_dtypes(include=['float64'])\n",
    "df_string_cols = df[df.columns.difference(dff.columns.values)]\n",
    "print df_string_cols.columns.values\n",
    "#display(dff.describe())\n",
    "print (np.abs(st.zscore(dff, axis=1)) > 3)\n",
    "#print dff.sub(dff.mean()).div(dff.std()).abs().lt(3)\n",
    "df_no_outliers = dff[dff.sub(dff.mean()).div(dff.std()).abs().lt(4)]\n",
    "df_no_outliers[df_string_cols.columns.values] = df_string_cols\n",
    "if verbose:\n",
    "    display(df_no_outliers.describe(include=\"all\"))\n",
    "df = df_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove variables related to lab times (i.e. *.HRS_2_SURGERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variables that have to do with time\n",
    "# cols_to_keep_no_hrs2surgery = [c for c in df.columns if not c.endswith(\".HRS_2_SURGERY\")]\n",
    "# print cols_to_keep_no_hrs2surgery\n",
    "# print len(cols_to_keep_no_hrs2surgery)\n",
    "# df=df[cols_to_keep_no_hrs2surgery]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unnecessary features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this for checking predictions over time\n",
    "admsn_surgery_number = df[\"ADMSN_SURGERY_NUMBER\"]\n",
    "print admsn_surgery_number.shape\n",
    "or_case_id_number = df[\"OR_CASE_ID\"]\n",
    "admsn_ids = df['ADMSN_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[final_features + [TARGET_VARIABLE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in cat_vars:\n",
    "    try:\n",
    "        # drop_first uses k-1 dummies out of k categories\n",
    "        print var\n",
    "        df = pd.get_dummies(df, columns=[var], drop_first=True)\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass\n",
    "# remove categorical variables (string values)\n",
    "for var in cat_vars:\n",
    "    try:\n",
    "        df.drop(var, axis=1, inplace=True)\n",
    "        pass\n",
    "    except ValueError:\n",
    "        print var, 'already dropped'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove features we don't want to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(feature_whitelist) == 0:\n",
    "for cat in cat_to_drop:\n",
    "    try:\n",
    "        df.drop(cat, axis=1, inplace=True)\n",
    "    except ValueError:\n",
    "        print cat, 'already dropped'\n",
    "#print df.columns.values\n",
    "for col in sorted(df.columns.values):\n",
    "    print col, \"\\t\\t\", df[col].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove target variables from data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Column names:\", df.columns.values\n",
    "try:\n",
    "    y = np.ravel(df[TARGET_VARIABLE])\n",
    "    #asa_status = df[\"ASA_STATUS\"]\n",
    "    df.drop(TARGET_VARIABLE, axis=1, inplace=True, errors='ignore')\n",
    "    df.drop(outcome_vars, axis=1, inplace=True, errors='ignore')\n",
    "    input_death_yn = df['INPT_DEATH_YN']\n",
    "except KeyError:\n",
    "    print TARGET_VARIABLE, \"already dropped\"\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "# default stragegy: mean\n",
    "# if len(feature_whitelist) > 0:\n",
    "#     feature_whitelist = [c for c in feature_whitelist if not c.endswith(\".HRS_2_SURGERY\")]\n",
    "#     df = df[feature_whitelist]\n",
    "print df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize training, testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardizeWithNaN(TransformerMixin, BaseEstimator):\n",
    "    '''This estimator is for standardizing a dataset that has missing data'''\n",
    "    def __init__(self):\n",
    "        self.X_mean = []\n",
    "        self.X_std = []\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # get mean and standard deviation of columns\n",
    "        self.X_mean = np.nanmean(X, axis=0)\n",
    "        self.X_std  = np.nanstd(X, axis=0) \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # subtract mean and divide by standard deviation\n",
    "        return (X - self.X_mean)/self.X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardizeWithNaN()\n",
    "#scaler.fit(X_test)\n",
    "#X_test = scaler.transform(X_test)\n",
    "scaler.fit(df)\n",
    "X_test = scaler.transform(df)\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftImputeEstimator(TransformerMixin, BaseEstimator):\n",
    "    '''This estimator is for wrapping the SoftImpute algorithm'''\n",
    "    def __init__(self, max_iters=200, verbose=True):\n",
    "        self.max_iters = max_iters\n",
    "        self.verbose = verbose\n",
    "        self.fit_count = 0\n",
    "        self.transform_count = 0\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_count += 1\n",
    "        print(\"SoftImputeEstimator fit count: {}\".format(self.fit_count))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.transform_count += 1\n",
    "        print(\"SoftImputeEstimator transform count: {}\".format(self.transform_count))\n",
    "        try:\n",
    "            # subtract mean and divide by standard deviation\n",
    "            return SoftImpute(max_iters=self.max_iters, verbose=self.verbose).complete(X.replace(np.inf, np.nan))\n",
    "        # ValueError raised if no values need to be imputed\n",
    "        except ValueError:\n",
    "            return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"imputing X_test\")\n",
    "#print(np.isnan(X_test).any())\n",
    "si = SoftImputeEstimator()\n",
    "X_test = si.transform(X_test.replace(np.inf, np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}\n",
    "models = []\n",
    "for i in range(1, num_cross_val_folds+1):\n",
    "    model_file_name = \"Random Forest_fold{}_sk{}.pkl\".format(i, sk_version)\n",
    "    print \"Loaded\", model_file_name\n",
    "    model = pickle.load(open(os.path.join(model_dir, model_file_name), \"rb\"))\n",
    "    #models[\"fold_{}\".format(i)] = model\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict classes and get probability of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_predictions = {k: model.predict(np.array(X_test)) for k, model in models.items()}\n",
    "# model_probs = {k: model.predict_proba(X_test) for k, model in models.items()}\n",
    "\n",
    "model_predictions = [model.predict(np.array(X_test)) for model in models]\n",
    "model_probs = [model.predict_proba(X_test) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_probs_df = pd.DataFrame.from_dict({k: probs[:,1] for k, probs in model_probs.items()})\n",
    "# model_probs_df.std(axis=1).plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we have true labels, see how well the model did "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"fold_{}\".format(i) for i in range(1, num_cross_val_folds+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_val_roc_curve(model_probs, y_test, \"test_cross_val_roc.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_accuracy_roc_auc(models, model_names, model_predictions, model_probs, y_train, y_test, \"test_accuracy_roc_auc.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(models, model_names, model_probs, y_test, os.path.join(dir_to_save_files,\"test_roc_curve.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(models, model_names, model_probs, y_test, os.path.join(dir_to_save_files, \"test_precision_recall_curve.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ehr]",
   "language": "python",
   "name": "conda-env-ehr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
